{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "from utils import *\n",
    "from models import instructir\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "from text.models import LanguageModel, LMHead\n",
    "\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
    "# DO you want inpainting or not?\n",
    "\n",
    "# if you do set the following to True\n",
    "i_want_inpainting = True  # if model doesnt exists if you try, remind wouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the config file at `configs/eval5d.yml` for more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG     = \"configs/eval5d.yml\"\n",
    "USE_WANDB  = False\n",
    "LM_MODEL   = \"models/lm_instructir-7d.pt\"\n",
    "# MODEL_NAME = \"models/im_instructir-7d.pt\"\n",
    "if i_want_inpainting:\n",
    "    MODEL_NAME = hf_hub_download(repo_id=\"Wouter01/InstructIR_with_inpainting\", filename=\"best_model.pt\")\n",
    "else:\n",
    "    MODEL_NAME = hf_hub_download(repo_id=\"Wouter01/instructir_hard_data\", filename=\"instructir_hard_data/best_model.pt\")\n",
    "\n",
    "# parse config file\n",
    "with open(os.path.join(CONFIG), \"r\") as f:\n",
    "    config = yaml.safe_load(f) \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = dict2namespace(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the InstructIR model using the provided pre-trained weights at `models/`.\n",
    "- You will need to load the image model `im_`\n",
    "- and the language model `lm_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Creating InstructIR\")\n",
    "model = instructir.create_model(input_channels =cfg.model.in_ch, width=cfg.model.width, enc_blks = cfg.model.enc_blks, \n",
    "                            middle_blk_num = cfg.model.middle_blk_num, dec_blks = cfg.model.dec_blks, txtdim=cfg.model.textdim)\n",
    "\n",
    "################### LOAD IMAGE MODEL\n",
    "\n",
    "assert MODEL_NAME, \"Model weights required for evaluation\"\n",
    "\n",
    "print (\"IMAGE MODEL CKPT:\", MODEL_NAME)\n",
    "model.load_state_dict(torch.load(MODEL_NAME, map_location=device), strict=True)\n",
    "\n",
    "nparams   = count_params (model)\n",
    "print (\"Loaded weights!\", nparams / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### LANGUAGE MODEL\n",
    "\n",
    "if cfg.model.use_text:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # Initialize the LanguageModel class\n",
    "    LMODEL = cfg.llm.model\n",
    "    language_model = LanguageModel(model=LMODEL)\n",
    "    lm_head = LMHead(embedding_dim=cfg.llm.model_dim, hidden_dim=cfg.llm.embd_dim, num_classes=cfg.llm.nclasses)\n",
    "    lm_head = lm_head #.to(device)\n",
    "    lm_nparams   = count_params (lm_head)\n",
    "\n",
    "    print (\"LMHEAD MODEL CKPT:\", LM_MODEL)\n",
    "    lm_head.load_state_dict(torch.load(LM_MODEL, map_location=device), strict=True)\n",
    "    print (\"Loaded weights!\")\n",
    "\n",
    "else:\n",
    "    LMODEL = None\n",
    "    language_model = None\n",
    "    lm_head = None\n",
    "    lm_nparams = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running InstructIR! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img (image, prompt, iterations=1):\n",
    "    \"\"\"\n",
    "    Given an image and a prompt, we run InstructIR to restore the image following the human prompt.\n",
    "    image: RGB image as numpy array normalized to [0,1]\n",
    "    prompt: plain python string,\n",
    "\n",
    "    returns the restored image as numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to tensor\n",
    "    y = torch.Tensor(image).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "    # Get the text embedding (and predicted degradation class)\n",
    "    lm_embd = language_model(prompt)\n",
    "    lm_embd = lm_embd #.to(device)\n",
    "    text_embd, deg_pred = lm_head (lm_embd)\n",
    "\n",
    "    # Forward pass: Paper Figure 2\n",
    "    for _ in range(iterations):\n",
    "        y = model(y, text_embd)\n",
    "\n",
    "    # convert the restored image <x_hat> into a np array\n",
    "    restored_img = y[0].permute(1,2,0).cpu().detach().numpy()\n",
    "    restored_img = np.clip(restored_img, 0. , 1.)\n",
    "    return restored_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to feed into the function and RGB image and a simple instruction (string).\n",
    "\n",
    "Check `load_img` from `utils.py` we just use PIL to load a given image.\n",
    "You can find more sample images in `images/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_img = [\"images/blurry_man.jpg\",\n",
    "            \"images/blurry_room.jpg\",\n",
    "            \"images/blurry_wall.jpg\",\n",
    "            \"images/bad_text.jpg\"]\n",
    "\n",
    "demo_img_inpainting = [\"images/black_everywhere.jpg\",\n",
    "            \"images/missing_kitchen.jpg\",\n",
    "            \"images/blurry_blackcoat.jpg\",\n",
    "            \"images/missing_room.jpg\"]\n",
    "\n",
    "images = demo_img_inpainting if i_want_inpainting else demo_img\n",
    "\n",
    "for IMG in images:\n",
    "    \n",
    "    PROMPT = \"Please make the image crispier and sharper\"\n",
    "    # NOTE we finetuned on this prompt, others may not work as well\n",
    "\n",
    "    image = load_img(IMG)\n",
    "    restored_image = process_img(image, PROMPT, iterations=1)  # trained for 1 iteration\n",
    "    # save_rgb (restored_image, \"result.png\") # save the resultant image\n",
    "    plot_all([image, restored_image], names=[\"Before\", \"After\"], figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG    = \"images/frog.png\" # from RealSRSet\n",
    "PROMPT = \"Can you remove the little dots in the image? is very unpleasant\"\n",
    "\n",
    "image = load_img(IMG)\n",
    "restored_image = process_img(image, PROMPT)\n",
    "# save_rgb (restored_image, \"result.png\") # save the resultant image\n",
    "plot_all([image, restored_image], names=[\"Before\", \"After\"], figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the process\n",
    "\n",
    "Sometimes the blur, rain, or film grain noise are pleasant effects and part of the **\"aesthetics\"**.\n",
    "\n",
    "Here we show a simple example on how to interact with InstructIR \n",
    "> Disclaimer: please remember this is not a product, thus, you will notice some limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG    = \"images/rain-020.png\"\n",
    "image  = load_img(IMG)\n",
    "\n",
    "PROMPT1 = \"I love this photo, could you remove the raindrops? please keep the content intact\"\n",
    "result1 = process_img(image, PROMPT1 )\n",
    "plot_all([image, result1], figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT2 = \"Can you make it look stunning? like a professional photo\"\n",
    "result2 = process_img(result1, PROMPT2 )\n",
    "plot_all([result1, result2], figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result looks indeed stunning :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all([image, result1, result2], names=[\"Input\", \"Prompt 1 - Result\", \"Prompt 2 - Result\"], figsize=(14,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
